{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import os, glob, torch, cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from math import sqrt\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import transforms as T\n",
    "from types import SimpleNamespace as Namespace  # cleaner than argparse\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "sys.path.append(os.getcwd() + \"/ldm\")\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "\n",
    "transform_PIL = T.ToPILImage()\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def create_model(device, yaml_path, model_path):\n",
    "    config = OmegaConf.load(yaml_path)\n",
    "    config.model['params']['ckpt_path'] = model_path\n",
    "    model = instantiate_from_config(config.model)\n",
    "    sampler = DDIMSampler(model)\n",
    "    model = model.to(device)\n",
    "    return model, sampler\n",
    "\n",
    "def process_data(image_pth, mask_pth, kernel_size=2):\n",
    "    mask = cv2.imread(mask_pth, cv2.IMREAD_GRAYSCALE)\n",
    "    original_size = mask.shape\n",
    "\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    dilated_mask = cv2.dilate(mask, kernel, iterations=1)\n",
    "    dilated_mask = Image.fromarray(dilated_mask)\n",
    "\n",
    "    dilated_mask = np.expand_dims(dilated_mask, axis=2).astype(np.float32) / 255.0\n",
    "    dilated_mask = (dilated_mask >= 0.1).astype(np.float32)\n",
    "    dilated_mask = torch.from_numpy(dilated_mask.transpose(2,0,1)[None])\n",
    "\n",
    "    image = np.array(Image.open(image_pth).convert(\"RGB\").resize((512,512))).astype(np.float32) / 255.0\n",
    "    image = torch.from_numpy(image.transpose(2,0,1)[None])\n",
    "\n",
    "    mask = np.array(Image.open(mask_pth).convert(\"L\").resize((512,512))).astype(np.float32) / 255.0\n",
    "    mask = (mask >= 0.1).astype(np.float32)\n",
    "    mask = torch.from_numpy(mask[None,None])\n",
    "\n",
    "    masked_image = (1 - mask) * image\n",
    "\n",
    "    batch = {\"image\": image * 2.0 - 1.0, \n",
    "             \"mask\": dilated_mask * 2.0 - 1.0, \n",
    "             \"masked_image\": masked_image * 2.0 - 1.0}\n",
    "\n",
    "    original_image = Image.open(image_pth).convert(\"RGB\")\n",
    "    original_mask = Image.open(mask_pth).convert(\"L\")\n",
    "\n",
    "    imagename = os.path.splitext(os.path.basename(image_pth))[0]\n",
    "    \n",
    "    return batch, original_size, original_image, original_mask, imagename\n",
    "\n",
    "def run_inference(args):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else \n",
    "                          'mps' if getattr(torch.backends, 'mps', None) and torch.backends.mps.is_available() \n",
    "                          else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    if not os.path.exists(args.log_path):\n",
    "        os.makedirs(args.log_path)\n",
    "    \n",
    "    model, sampler = create_model(device, args.yaml_path, args.model_path)\n",
    "    model.eval()\n",
    "\n",
    "    logpath = os.path.join(args.log_path, args.image.split('/')[-1].replace('.', '_'))\n",
    "    os.makedirs(logpath, exist_ok=True)\n",
    "\n",
    "    batch, original_size, original_image, original_mask, imagename = process_data(args.image, args.mask, args.dilate_kernel)\n",
    "\n",
    "    c = model.cond_stage_model.encode(batch[\"masked_image\"].to(device))\n",
    "    cc = torch.nn.functional.interpolate(batch[\"mask\"].to(device), size=c.shape[-2:])\n",
    "    c = torch.cat((c, cc), dim=1)\n",
    "\n",
    "    shape = (c.shape[1]-1,) + c.shape[2:]\n",
    "    cond = c.expand(args.batchsize, -1, -1, -1)\n",
    "\n",
    "    samples_ddim, _ = sampler.sample(\n",
    "        S=args.Steps,\n",
    "        conditioning=cond,\n",
    "        batch_size=cond.shape[0],\n",
    "        shape=shape,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "    predicted_image_clamped = torch.clamp((x_samples_ddim + 1.0) / 2.0, 0.0, 1.0)\n",
    "\n",
    "    all_samples = []\n",
    "    base_count = len(glob.glob(os.path.join(logpath, 'sample*')))\n",
    "    grid_count = len(glob.glob(os.path.join(logpath, 'grid*')))\n",
    "\n",
    "    image_array = np.array(original_image)\n",
    "    mask_array = np.array(original_mask)\n",
    "\n",
    "    for sample in predicted_image_clamped:\n",
    "        output_PIL = transform_PIL(sample)\n",
    "        output_PIL = output_PIL.resize((original_size[1], original_size[0]))\n",
    "\n",
    "        if args.isReplace:\n",
    "            out_array = np.array(output_PIL)\n",
    "            out_array[mask_array == 0] = image_array[mask_array == 0]\n",
    "            output_PIL = Image.fromarray(out_array)\n",
    "\n",
    "        output_PIL.save(os.path.join(logpath, f\"sample_{base_count:05}.png\"))\n",
    "        base_count += 1\n",
    "\n",
    "    all_samples.append(predicted_image_clamped)\n",
    "\n",
    "    # Save grid\n",
    "    grid = torch.stack(all_samples, 0)\n",
    "    grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "    grid = make_grid(grid, nrow=int(sqrt(args.batchsize)))\n",
    "\n",
    "    grid_img = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "    grid_img = Image.fromarray(grid_img.astype(np.uint8))\n",
    "    grid_img.save(os.path.join(logpath, f'grid-{grid_count:04}.png'))\n",
    "\n",
    "    print(f\"Results saved to {logpath}\")\n",
    "\n",
    "# --- Example Manual args (for Jupyter) ---\n",
    "args = Namespace(\n",
    "    yaml_path = \"ldm/models/ldm/inpainting_big/config_LAKERED.yaml\",\n",
    "    model_path = \"ckpt/LAKERED.ckpt\",\n",
    "    log_path = \"demo_res\",\n",
    "    image = \"demo/src/COD_CAMO_camourflage_00018.jpg\",\n",
    "    mask = \"demo/src/COD_CAMO_camourflage_00018.png\",\n",
    "    batchsize = 1,\n",
    "    isReplace = False,\n",
    "    dilate_kernel = 2,\n",
    "    Steps = 50\n",
    ")\n",
    "\n",
    "run_inference(args)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
